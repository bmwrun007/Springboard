{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#put in the code that made 'uniqueDirections' so that it's available\n",
    "#see what objects you can delete so that this whole thing goes faster\n",
    "#triple check that interpolation is working on multiple bike rides at once...\n",
    "#try to increase resolution of video\n",
    "#try and make a timer that shows what time it is during these frames... maybe a title?\n",
    "#TODO reexamine why you chose these dates... make sure you get all the rides on these dates (i.e. if we're trying to get 8/9, make sure we have rides that went from 8/8 to 8/9, and 8/9 into 8/10)\n",
    "#we either need to change the code such that phantomJS and ffmpeg are called directly or add them to the path\n",
    "\n",
    "\n",
    "#if we really want, maybe we can see if we can improve the bike interpolation\n",
    "#make this multithreaded so it goes faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import polyline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os.path import isdir\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import folium\n",
    "\n",
    "import subprocess\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\",1000)\n",
    "pd.set_option(\"display.max_columns\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_frames(df_final_rounded_time, date, pandas_date, alternate_range=None):\n",
    "    directory = 'Image Pipe//' + date + '//'\n",
    "    if not(os.path.isdir(directory)):\n",
    "         os.makedirs(directory)\n",
    "    if alternate_range is None:\n",
    "        dayInterval = pd.date_range(start=pandas_date, end=pandas_date + pd.DateOffset(1), freq='15s')\n",
    "    else:\n",
    "        dayInterval = alternate_range\n",
    "    i = 0\n",
    "    for interval in dayInterval:\n",
    "        #get rows that coincide with the time we're trying to plot\n",
    "        df_final_rounded_time_temp = df_final_rounded_time[df_final_rounded_time['Rounded Time']==interval]\n",
    "        #of those, get seperate rows where the bikes are finishing a ride, starting a ride, or in the middle of a ride\n",
    "        df_final_rounded_time_temp_start = df_final_rounded_time_temp[df_final_rounded_time_temp['Percent Complete']==0]\n",
    "        df_final_rounded_time_temp_end = df_final_rounded_time_temp[df_final_rounded_time_temp['Percent Complete']==1]\n",
    "        df_final_rounded_time_temp_middle = df_final_rounded_time_temp[(df_final_rounded_time_temp['Percent Complete']!=0) & (df_final_rounded_time_temp['Percent Complete']!=1)]\n",
    "        sg_map = folium.Map(location=[40.73, -73.97], tiles='Stamen Terrain', zoom_start=12)\n",
    "        #define how to deal with bikes who are in each of the three stages of a ride\n",
    "        def plotDotStart(point):\n",
    "            '''input: series that contains a numeric named latitude and a numeric named longitude\n",
    "            this function creates a CircleMarker and adds it to your this_map'''\n",
    "            folium.CircleMarker(location=[point['Point Latitude'], point['Point Longitude']],\n",
    "                                fill_color='red', radius=5, popup=str(point['Point Latitude']) + \" \" + str(point['Point Longitude']),\n",
    "                                weight=0, fill=True, fill_opacity=.3).add_to(sg_map)\n",
    "        def plotDotEnd(point):\n",
    "            '''input: series that contains a numeric named latitude and a numeric named longitude\n",
    "            this function creates a CircleMarker and adds it to your this_map'''\n",
    "            folium.CircleMarker(location=[point['Point Latitude'], point['Point Longitude']],\n",
    "                                fill_color='green', radius=5, popup=str(point['Point Latitude']) + \" \" + str(point['Point Longitude']),\n",
    "                                weight=0, fill=True, fill_opacity=.3).add_to(sg_map)\n",
    "        def plotDotMiddle(point):\n",
    "            '''input: series that contains a numeric named latitude and a numeric named longitude\n",
    "            this function creates a CircleMarker and adds it to your this_map'''\n",
    "            folium.CircleMarker(location=[point['Point Latitude'], point['Point Longitude']],\n",
    "                                fill_color='yellow', radius=5, popup=str(point['Point Latitude']) + \" \" + str(point['Point Longitude']),\n",
    "                                weight=0, fill=True, fill_opacity=.3).add_to(sg_map)\n",
    "        #apply appropriate plotting function to each dataframe\n",
    "        df_final_rounded_time_temp_start.apply(plotDotStart, axis = 1)\n",
    "        df_final_rounded_time_temp_end.apply(plotDotEnd, axis = 1)\n",
    "        df_final_rounded_time_temp_middle.apply(plotDotMiddle, axis = 1)\n",
    "        #save the map as html\n",
    "        sg_map.save(directory + str(i) + '.html')\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def html_to_png(date, rangeLength=5761):\n",
    "    #run program from the command line to convert html to png files\n",
    "    #cmdstringPhantom = ([\"phantomjs.exe\", \"convertToPNG-auto.js\"])\n",
    "    cmdstringPhantom = ([\"C://Users//Bryan//Documents//GitHub//Springboard//Capstone Project//Image Pipe//phantomjs.exe\", \"convertToPNG-auto.js\"])\n",
    "    myCWD = \"C://Users//Bryan//Documents//GitHub//Springboard//Capstone Project//Image Pipe//\" + date\n",
    "\n",
    "    for i in range(rangeLength):    \n",
    "        javaScriptCode = \"\"\"var page = require('webpage').create();\n",
    "        //viewportSize being the actual size of the headless browser\n",
    "        page.viewportSize = { width: 1024, height: 768 };\n",
    "        //the clipRect is the portion of the page you are taking a screenshot of\n",
    "        page.clipRect = { top: 0, left: 0, width: 1024, height: 768 };\n",
    "        //the rest of the code is the same as the previous example\n",
    "        page.open('\"\"\" + str(i) + \"\"\".html', function() {\n",
    "          page.render('\"\"\" + str(i) + \"\"\".png');\n",
    "          phantom.exit();\n",
    "        });\\n\"\"\"\n",
    "\n",
    "        target = open(myCWD + '//convertToPNG-auto.js', 'w')\n",
    "        target.write(javaScriptCode)\n",
    "        target.close()\n",
    "        subprocess.check_call(cmdstringPhantom, stdin=subprocess.PIPE, cwd=myCWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_video(date):\n",
    "    #run program from the command convert png files into an animation\n",
    "    # ffmpeg -f image2 -i %d.png -r 2 mybike.avi\n",
    "    myCWD = \"C://Users//Bryan//Documents//GitHub//Springboard//Capstone Project//Image Pipe//\" + date\n",
    "    #cmdstringFFMPEG = ([\"C:\\\\Users\\\\Bryan\\\\Documents\\\\GitHub\\\\Springboard\\\\Capstone Project\\\\Image Pipe\\\\FFMPEG.exe\", \"-f\", \"image2\", \"-i\", \"%d.png\", \"-r\", \"2\", \"mybike.avi\"])\n",
    "    videoString = pd.to_datetime(date).strftime('%Y%m%d') + \".avi\"\n",
    "    cmdstringFFMPEG = ([\"FFMPEG.exe\", \"-f\", \"image2\", \"-i\", \"%d.png\", \"-r\", \"2\", videoString])\n",
    "    subprocess.check_call(cmdstringFFMPEG, stdin=subprocess.PIPE, cwd=myCWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rangeLength=5761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find and pickle directions between all stations\n",
    "\n",
    "#prepare data for loading\n",
    "mypath = \"C:\\Users\\Bryan\\Documents\\GitHub\\Springboard\\Capstone Project\\Data\\\\\"\n",
    "dataFiles = [f for f in listdir(mypath) if isfile(join(mypath,f))]\n",
    "dataFrames = []\n",
    "\n",
    "#the names of the columns, but not the actual data, change from file to file, to establish\n",
    "#the names of the columns we want\n",
    "inputColumns = ['Trip Duration', 'Start Time', 'Stop Time', 'Start Station ID', 'Start Station Name', 'Start Station Latitude',\n",
    "                'Start Station Longitude', 'End Station ID', 'End Station Name', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'User Type', 'Birth Year', 'Gender']\n",
    "\n",
    "#use to help the read_csv file determine datatypes\n",
    "inputDtypeDict = {'Trip Duration': np.int64,\n",
    "                  #'starttime': np.datetime64,\n",
    "                  #'stoptime': np.datetime64,\n",
    "                  'Start Station ID': np.int64,\n",
    "                  'Start Station Name': np.string_,\n",
    "                  'Start Station Latitude': np.float64,\n",
    "                  'Start Station Longitude': np.float64,\n",
    "                  'End Station ID': np.int64,\n",
    "                  'End Station Name': np.string_,\n",
    "                  'End Station Latitude': np.float64,\n",
    "                  'End Station Longitude': np.float64,\n",
    "                  'Bike ID': np.int64,\n",
    "                  'User Type': np.string_,\n",
    "                  'Birth Year': np.string_,\n",
    "                  'Gender': np.int64,\n",
    "                  'Start Next Station Id': np.int64\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2013-07 - Citi Bike trip data.csv', '2013-08 - Citi Bike trip data.csv', '2013-09 - Citi Bike trip data.csv', '2013-10 - Citi Bike trip data.csv', '2013-11 - Citi Bike trip data.csv', '2013-12 - Citi Bike trip data.csv', '2014-01 - Citi Bike trip data.csv', '2014-02 - Citi Bike trip data.csv', '2014-03 - Citi Bike trip data.csv', '2014-04 - Citi Bike trip data.csv', '2014-05 - Citi Bike trip data.csv', '2014-06 - Citi Bike trip data.csv', '2014-07 - Citi Bike trip data.csv', '2014-08 - Citi Bike trip data.csv', '201409-citibike-tripdata.csv', '201410-citibike-tripdata.csv', '201411-citibike-tripdata.csv', '201412-citibike-tripdata.csv', '201501-citibike-tripdata.csv', '201502-citibike-tripdata.csv', '201503-citibike-tripdata.csv', '201504-citibike-tripdata.csv', '201505-citibike-tripdata.csv', '201506-citibike-tripdata.csv', '201507-citibike-tripdata.csv', '201508-citibike-tripdata.csv', '201509-citibike-tripdata.csv', '201510-citibike-tripdata.csv', '201511-citibike-tripdata.csv', '201512-citibike-tripdata.csv', '201601-citibike-tripdata.csv', '201602-citibike-tripdata.csv', '201603-citibike-tripdata.csv', '201604-citibike-tripdata.csv', '201605-citibike-tripdata.csv', '201606-citibike-tripdata.csv', '201607-citibike-tripdata.csv', '201608-citibike-tripdata.csv', '201609-citibike-tripdata.csv', '201610-citibike-tripdata.csv', '201611-citibike-tripdata.csv', '201612-citibike-tripdata.csv', '201701-citibike-tripdata.csv', '201702-citibike-tripdata.csv', '201703-citibike-tripdata.csv', '201704-citibike-tripdata.csv', '201705-citibike-tripdata.csv', '201706-citibike-tripdata.csv', '201707-citibike-tripdata.csv']\n"
     ]
    }
   ],
   "source": [
    "print dataFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 2013-07 - Citi Bike trip data.csv has 843416 records\n",
      "File 2013-08 - Citi Bike trip data.csv has 1001958 records\n",
      "File 2013-09 - Citi Bike trip data.csv has 1034359 records\n",
      "File 2013-10 - Citi Bike trip data.csv has 1037712 records\n",
      "File 2013-11 - Citi Bike trip data.csv has 675774 records\n",
      "File 2013-12 - Citi Bike trip data.csv has 443966 records\n",
      "File 2014-01 - Citi Bike trip data.csv has 300400 records\n",
      "File 2014-02 - Citi Bike trip data.csv has 224736 records\n",
      "File 2014-03 - Citi Bike trip data.csv has 439117 records\n",
      "File 2014-04 - Citi Bike trip data.csv has 670780 records\n",
      "File 2014-05 - Citi Bike trip data.csv has 866117 records\n",
      "File 2014-06 - Citi Bike trip data.csv has 936880 records\n",
      "File 2014-07 - Citi Bike trip data.csv has 968842 records\n",
      "File 2014-08 - Citi Bike trip data.csv has 963489 records\n",
      "File 201409-citibike-tripdata.csv has 953887 records\n",
      "File 201410-citibike-tripdata.csv has 828711 records\n",
      "File 201411-citibike-tripdata.csv has 529188 records\n",
      "File 201412-citibike-tripdata.csv has 399069 records\n",
      "File 201501-citibike-tripdata.csv has 285552 records\n",
      "File 201502-citibike-tripdata.csv has 196930 records\n",
      "File 201503-citibike-tripdata.csv has 341826 records\n",
      "File 201504-citibike-tripdata.csv has 652390 records\n",
      "File 201505-citibike-tripdata.csv has 961986 records\n",
      "File 201506-citibike-tripdata.csv has 941219 records\n",
      "File 201507-citibike-tripdata.csv has 1085676 records\n",
      "File 201508-citibike-tripdata.csv has 1179044 records\n",
      "File 201509-citibike-tripdata.csv has 1289699 records\n",
      "File 201510-citibike-tripdata.csv has 1212277 records\n",
      "File 201511-citibike-tripdata.csv has 987245 records\n",
      "File 201512-citibike-tripdata.csv has 804125 records\n",
      "File 201601-citibike-tripdata.csv has 509478 records\n",
      "File 201602-citibike-tripdata.csv has 560874 records\n",
      "File 201603-citibike-tripdata.csv has 919921 records\n",
      "File 201604-citibike-tripdata.csv has 1013149 records\n",
      "File 201605-citibike-tripdata.csv has 1212280 records\n",
      "File 201606-citibike-tripdata.csv has 1460318 records\n",
      "File 201607-citibike-tripdata.csv has 1380110 records\n",
      "File 201608-citibike-tripdata.csv has 1557663 records\n",
      "File 201609-citibike-tripdata.csv has 1648856 records\n",
      "File 201610-citibike-tripdata.csv has 1573872 records\n",
      "File 201611-citibike-tripdata.csv has 1196942 records\n",
      "File 201612-citibike-tripdata.csv has 812192 records\n",
      "File 201701-citibike-tripdata.csv has 726676 records\n",
      "File 201702-citibike-tripdata.csv has 791647 records\n",
      "File 201703-citibike-tripdata.csv has 727665 records\n",
      "File 201704-citibike-tripdata.csv has 1315404 records\n",
      "File 201705-citibike-tripdata.csv has 1523268 records\n",
      "File 201706-citibike-tripdata.csv has 1731594 records\n",
      "File 201707-citibike-tripdata.csv has 1735599 records\n"
     ]
    }
   ],
   "source": [
    "#load data into n dataframes\n",
    "for f in dataFiles:\n",
    "    data_df = pd.read_csv(mypath + f, header=0, names=inputColumns, dtype=inputDtypeDict, parse_dates=['Start Time', 'Stop Time'], infer_datetime_format=True)\n",
    "    print \"File \" + f + \" has \" + str(len(data_df)) + \" records\"\n",
    "    dataFrames.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final file has 45453878 records\n"
     ]
    }
   ],
   "source": [
    "#join all dataframes into single dataframe\n",
    "allTrips_df = pd.concat(dataFrames)\n",
    "print \"Final file has \" + str(len(allTrips_df)) + \" records\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allTrips_df['Start Date'] = allTrips_df['Start Time'].dt.date\n",
    "allTrips_df['Start Date'] = pd.to_datetime(allTrips_df['Start Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniqueDirections_df_part1 = pd.read_pickle('additional_trips1.pickle')\n",
    "uniqueDirections_df_part2 = pd.read_pickle('additional_trips2.pickle')\n",
    "uniqueDirections_df_part3 = pd.read_pickle('additional_trips3.pickle')\n",
    "uniqueDirections_df = pd.concat([uniqueDirections_df_part1, uniqueDirections_df_part2, uniqueDirections_df_part3])\n",
    "uniqueDirections_df.rename(columns={'point latitude': 'Point Latitude',\n",
    "                                    'point longitude': 'Point Longitude',\n",
    "                                    'start station latitude': 'Start Station Latitude',\n",
    "                                    'start station longitude': 'Start Station Longitude',\n",
    "                                    'end station latitude': 'End Station Latitude',\n",
    "                                    'end station longitude': 'End Station Longitude'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of duplicate points on same trip... if you redump the data you can solve this problem ahead of time\n",
    "uniqueDirections_df = uniqueDirections_df.groupby(['Point Latitude', \n",
    "                             'Point Longitude', \n",
    "                             'Start Station Latitude', \n",
    "                             'Start Station Longitude', \n",
    "                             'End Station Latitude', \n",
    "                             'End Station Longitude'])['order'].min().to_frame().reset_index().\\\n",
    "                                sort_values(['Start Station Latitude',\n",
    "                               'Start Station Longitude',\n",
    "                               'End Station Latitude',\n",
    "                               'End Station Longitude',\n",
    "                               'order']).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Point Latitude</th>\n",
       "      <th>Point Longitude</th>\n",
       "      <th>Start Station Latitude</th>\n",
       "      <th>Start Station Longitude</th>\n",
       "      <th>End Station Latitude</th>\n",
       "      <th>End Station Longitude</th>\n",
       "      <th>order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.646301</td>\n",
       "      <td>-74.016839</td>\n",
       "      <td>40.646538</td>\n",
       "      <td>-74.016588</td>\n",
       "      <td>40.646538</td>\n",
       "      <td>-74.016588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.646811</td>\n",
       "      <td>-74.016169</td>\n",
       "      <td>40.646678</td>\n",
       "      <td>-74.016303</td>\n",
       "      <td>40.646678</td>\n",
       "      <td>-74.016303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.646920</td>\n",
       "      <td>-74.016350</td>\n",
       "      <td>40.646768</td>\n",
       "      <td>-74.016510</td>\n",
       "      <td>40.646768</td>\n",
       "      <td>-74.016510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.646920</td>\n",
       "      <td>-74.016350</td>\n",
       "      <td>40.646768</td>\n",
       "      <td>-74.016510</td>\n",
       "      <td>40.668546</td>\n",
       "      <td>-73.993333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.643988</td>\n",
       "      <td>-74.011482</td>\n",
       "      <td>40.646768</td>\n",
       "      <td>-74.016510</td>\n",
       "      <td>40.668546</td>\n",
       "      <td>-73.993333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Point Latitude  Point Longitude  Start Station Latitude  \\\n",
       "0       40.646301       -74.016839               40.646538   \n",
       "1       40.646811       -74.016169               40.646678   \n",
       "2       40.646920       -74.016350               40.646768   \n",
       "3       40.646920       -74.016350               40.646768   \n",
       "4       40.643988       -74.011482               40.646768   \n",
       "\n",
       "   Start Station Longitude  End Station Latitude  End Station Longitude  order  \n",
       "0               -74.016588             40.646538             -74.016588      0  \n",
       "1               -74.016303             40.646678             -74.016303      0  \n",
       "2               -74.016510             40.646768             -74.016510      0  \n",
       "3               -74.016510             40.668546             -73.993333      0  \n",
       "4               -74.016510             40.668546             -73.993333      1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueDirections_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.    \n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniqueDirections_df['dist'] = \\\n",
    "    haversine_np(uniqueDirections_df['Point Longitude'].shift(), uniqueDirections_df['Point Latitude'].shift(),\n",
    "                 uniqueDirections_df.loc[1:, 'Point Longitude'], uniqueDirections_df.loc[1:, 'Point Latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniqueDirections_df['dist'] = np.where(uniqueDirections_df['order']==0, 0, uniqueDirections_df['dist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniqueDirections_df['cumdist'] = uniqueDirections_df.groupby(['Start Station Latitude', \n",
    "                  'Start Station Longitude', \n",
    "                  'End Station Latitude', \n",
    "                  'End Station Longitude'])['dist'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalTravelDistance = uniqueDirections_df.groupby(['Start Station Latitude', \n",
    "                  'Start Station Longitude', \n",
    "                  'End Station Latitude', \n",
    "                  'End Station Longitude'])['cumdist'].max().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of routes that start and end in the same place\n",
    "totalTravelDistance = totalTravelDistance[totalTravelDistance['cumdist']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalTravelDistance.rename(columns={'cumdist':'Total Route Distance'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniqueDirections_df = pd.merge(uniqueDirections_df, totalTravelDistance, how='inner', \n",
    "                               on=['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniqueDirections_df['Percent Complete'] = uniqueDirections_df['cumdist'] / uniqueDirections_df['Total Route Distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of points that are extremely close together (as they mess with interpolation later if they still exist)\n",
    "uniqueDirections_df = uniqueDirections_df[~((uniqueDirections_df['dist']< 1.0e-12) & (uniqueDirections_df['dist']!= 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's examine only some specific dates that we can later compare with each other\n",
    "allTrips = allTrips_df[(allTrips_df['Start Date']=='2013-08-08') |\n",
    "                       (allTrips_df['Start Date']=='2013-08-09') |\n",
    "                       (allTrips_df['Start Date']=='2013-08-10') |\n",
    "                       (allTrips_df['Start Date']=='2013-11-07') |\n",
    "                       (allTrips_df['Start Date']=='2013-11-08') |\n",
    "                       (allTrips_df['Start Date']=='2013-11-09') |\n",
    "                       (allTrips_df['Start Date']=='2014-02-06') |\n",
    "                       (allTrips_df['Start Date']=='2014-02-07') |\n",
    "                       (allTrips_df['Start Date']=='2014-02-08') |\n",
    "                       (allTrips_df['Start Date']=='2014-05-08') |\n",
    "                       (allTrips_df['Start Date']=='2014-05-09') |\n",
    "                       (allTrips_df['Start Date']=='2014-05-10') |\n",
    "                       (allTrips_df['Start Date']=='2014-08-07') |\n",
    "                       (allTrips_df['Start Date']=='2014-08-08') |\n",
    "                       (allTrips_df['Start Date']=='2014-08-09') |\n",
    "                       (allTrips_df['Start Date']=='2014-11-06') |\n",
    "                       (allTrips_df['Start Date']=='2014-11-07') |\n",
    "                       (allTrips_df['Start Date']=='2014-11-08') |\n",
    "                       (allTrips_df['Start Date']=='2015-02-05') |\n",
    "                       (allTrips_df['Start Date']=='2015-02-06') |\n",
    "                       (allTrips_df['Start Date']=='2015-02-07') |\n",
    "                       (allTrips_df['Start Date']=='2015-05-07') |\n",
    "                       (allTrips_df['Start Date']=='2015-05-08') |\n",
    "                       (allTrips_df['Start Date']=='2015-05-09') |\n",
    "                       (allTrips_df['Start Date']=='2015-08-06') |\n",
    "                       (allTrips_df['Start Date']=='2015-08-07') |\n",
    "                       (allTrips_df['Start Date']=='2015-08-08') |\n",
    "                       (allTrips_df['Start Date']=='2015-11-05') |\n",
    "                       (allTrips_df['Start Date']=='2015-11-06') |\n",
    "                       (allTrips_df['Start Date']=='2015-11-07') |\n",
    "                       (allTrips_df['Start Date']=='2016-02-04') |\n",
    "                       (allTrips_df['Start Date']=='2016-02-05') |\n",
    "                       (allTrips_df['Start Date']=='2016-02-06') |\n",
    "                       (allTrips_df['Start Date']=='2016-05-05') |\n",
    "                       (allTrips_df['Start Date']=='2016-05-06') |\n",
    "                       (allTrips_df['Start Date']=='2016-05-07') |\n",
    "                       (allTrips_df['Start Date']=='2016-08-11') |\n",
    "                       (allTrips_df['Start Date']=='2016-08-12') |\n",
    "                       (allTrips_df['Start Date']=='2016-08-13') |\n",
    "                       (allTrips_df['Start Date']=='2016-11-03') |\n",
    "                       (allTrips_df['Start Date']=='2016-11-04') |\n",
    "                       (allTrips_df['Start Date']=='2016-11-05') |\n",
    "                       (allTrips_df['Start Date']=='2017-02-02') |\n",
    "                       (allTrips_df['Start Date']=='2017-02-03') |\n",
    "                       (allTrips_df['Start Date']=='2017-02-04')]\n",
    "\n",
    "#TODO reexamine why you chose these dates... make sure you get all the rides on these dates (i.e. if we're trying to get 8/9, make sure we have rides that went from 8/8 to 8/9, and 8/9 into 8/10)\n",
    "datesList = ['2013-08-09', '2013-08-10', '2013-11-08', '2013-11-09', '2014-02-07', '2014-02-08', '2014-05-09', '2014-05-10', '2014-08-08', '2014-08-09', '2014-11-07', '2014-11-08', '2015-02-06', '2015-02-07', '2015-05-08', '2015-05-09', '2015-08-07', '2015-08-08', '2015-11-06', '2015-11-07', '2016-02-05', '2016-02-06', '2016-05-06', '2016-05-07', '2016-08-12', '2016-08-13', '2016-11-04', '2016-11-05', '2017-02-03', '2017-02-04']\n",
    "#datesList = ['2013-08-10', '2013-11-08', '2013-11-09', '2014-02-07', '2014-02-08', '2014-05-09', '2014-05-10', '2014-08-08', '2014-08-09', '2014-11-07', '2014-11-08', '2015-02-06', '2015-02-07', '2015-05-08', '2015-05-09', '2015-08-07', '2015-08-08', '2015-11-06', '2015-11-07', '2016-02-05', '2016-02-06', '2016-05-06', '2016-05-07', '2016-08-12', '2016-08-13', '2016-11-04', '2016-11-05', '2017-02-03', '2017-02-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we are...\n"
     ]
    }
   ],
   "source": [
    "#this is where I start having problems...\n",
    "print \"Here we are...\"\n",
    "rangeLength=5761\n",
    "datesList = ['2013-08-09', '2013-08-10', '2013-11-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in datesList:\n",
    "    pandas_date = pd.to_datetime(date)\n",
    "    alternate_range = pd.date_range(pandas_date, periods=rangeLength, freq='15s')\n",
    "    allTripsOneDay = allTrips_df[(allTrips_df['Start Date']==pandas_date) | (allTrips_df['Start Date']==(pandas_date - pd.DateOffset(1)))]\n",
    "    allTripsOneDay = allTripsOneDay.reset_index().drop('index', axis=1).reset_index()\n",
    "    allTripsOneDay.rename(columns={'index':'factor'}, inplace=True)\n",
    "    allTripsOneDay['factor'] = allTripsOneDay['factor'] * 2\n",
    "    df_merge = pd.merge(uniqueDirections_df, allTripsOneDay, how='inner', on=['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude'])\n",
    "    df_merge.sort_values(['Bike ID', 'Start Time', 'order'], inplace=True)\n",
    "    df_merge['Time'] = np.where(df_merge['order']==0, df_merge['Start Time'], pd.NaT)\n",
    "    df_merge['Time'] = pd.to_datetime(df_merge['Time'])\n",
    "    df_merge['Time'] = np.where(df_merge['Percent Complete']==1, df_merge['Stop Time'], df_merge['Time'])\n",
    "    df_merge['Time'] = (df_merge['Time'] - np.datetime64('2013-01-01')) / np.timedelta64(1, 's')\n",
    "    df_merge['interp index'] = df_merge['Percent Complete'] + df_merge['factor']\n",
    "    df_merge.set_index('interp index', inplace=True)\n",
    "    df_merge['Time'] = df_merge.groupby(['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', \n",
    "                  'Start Time', 'Stop Time', 'Bike ID'])[['Time']].apply(pd.DataFrame.interpolate, method='index')['Time']\n",
    "    df_merge['Time'] = df_merge['Time'] * np.timedelta64(1, 's') + np.datetime64('2013-01-01T00:00:01')\n",
    "    #make 30 second buckets with min\n",
    "    df_merge['Rounded Time'] = df_merge['Time'].dt.round('15s')\n",
    "    # df_merge.to_csv('temp6.csv')\n",
    "    df_merge_temp = df_merge[['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Time', 'Rounded Time', 'order']]\n",
    "    df_min_times = df_merge_temp[['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Time', 'Rounded Time']].groupby(['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Rounded Time'])['Time'].min().to_frame().reset_index()\n",
    "    df_merge_temp2 = pd.merge(df_merge_temp, df_min_times, on=['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Rounded Time', 'Time'])\n",
    "    df_merge_temp3 = df_merge_temp2.groupby(['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Rounded Time', 'Time'])['order'].min().to_frame().reset_index()\n",
    "    df_merge_min = pd.merge(df_merge_temp3, df_merge_temp2, how='inner', on=['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Rounded Time', 'Time', 'order'])\n",
    "    df_end_points = df_merge[df_merge['Percent Complete']==1].copy()[['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Time', 'Rounded Time', 'order']]\n",
    "    df_min_with_end_points = pd.concat([df_merge_min, df_end_points])\n",
    "    df_min_with_max_end_points = df_min_with_end_points[['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Rounded Time', 'Time']].groupby(['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Rounded Time'])['Time'].max().to_frame().reset_index()\n",
    "    df_min_with_max_end_points2 = pd.merge(df_min_with_end_points, df_min_with_max_end_points, how='inner', on=['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Rounded Time', 'Time'])\n",
    "    df_min_with_max_end_points3 = df_min_with_max_end_points2.groupby(['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Rounded Time', 'Time'])['order'].max().to_frame().reset_index()\n",
    "    df_min_with_max_end_points4 = pd.merge(df_min_with_max_end_points3, df_min_with_max_end_points2, how='inner', on=['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Rounded Time', 'Time', 'order'])\n",
    "    df_final_rounded_time = pd.merge(df_min_with_max_end_points4, df_merge, how='inner', on=['Start Station Latitude', 'Start Station Longitude', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'Rounded Time', 'Time', 'order'])\n",
    "    df_final_rounded_time['Transit Date'] = df_final_rounded_time['Rounded Time'].dt.date\n",
    "    df_final_rounded_time['Transit Date'] = pd.to_datetime(df_final_rounded_time['Transit Date'])\n",
    "    df_final_rounded_time = df_final_rounded_time[df_final_rounded_time['Transit Date']==date]\n",
    "    print_frames(df_final_rounded_time, date, pandas_date, alternate_range)\n",
    "    html_to_png(date, rangeLength)\n",
    "    make_video(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write code for rebalancing, repeat above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#isolate records that indicate that a rebalancing occured\n",
    "#allTrips_df.sort_values(['bikeid', 'starttime'], inplace=True)\n",
    "#allTrips_df['start next station id'] = allTrips_df['start station id'].shift(-1)\n",
    "#rebalancing_df = allTrips_df[allTrips_df['end station id'] != allTrips_df['start next station id']].copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
